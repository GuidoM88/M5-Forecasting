{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f24b5e-d79b-40c7-8dde-3106fc5222c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTM M5 Multivariate\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Subset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import (\n",
    "    ForecastDFDataset,\n",
    "    TimeSeriesPreprocessor,\n",
    "    TinyTimeMixerForPrediction,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    ")\n",
    "from tsfm_public.toolkit.time_series_preprocessor import prepare_data_splits\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cpu\"  \n",
    "\n",
    "FORECAST_LENGTH = 28\n",
    "CONTEXT_LENGTH = 90  \n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "REVISION = \"90-30-ft-l1-r2.1\"  \n",
    "\n",
    "data_dir = Path('../data/raw')\n",
    "output_dir = Path('../data/ttm_m5_multivariate')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_SERIES_SAMPLE = 2000\n",
    "FEWSHOT_FRACTION = 0.10\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 256  \n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"Context: {CONTEXT_LENGTH} | Horizon: {FORECAST_LENGTH}\")\n",
    "print(f\"Series: {N_SERIES_SAMPLE} | Epochs: {NUM_EPOCHS} | Device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "sales = pd.read_csv(data_dir / 'sales_train_evaluation.csv')\n",
    "calendar = pd.read_csv(data_dir / 'calendar.csv')\n",
    "prices = pd.read_csv(data_dir / 'sell_prices.csv')\n",
    "print(f\"  Loaded: Sales {sales.shape} | Calendar {calendar.shape} | Prices {prices.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare Calendar - VECTORIZED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Preparing calendar...\")\n",
    "calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "calendar['wday_num'] = calendar['wday'].astype('category').cat.codes\n",
    "calendar['month_num'] = calendar['month'].astype('category').cat.codes\n",
    "calendar['year_norm'] = calendar['year'] - 2011\n",
    "calendar['is_weekend'] = calendar['weekday'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "calendar['has_event_1'] = calendar['event_name_1'].notna().astype(int)\n",
    "calendar['has_event_2'] = calendar['event_name_2'].notna().astype(int)\n",
    "\n",
    "cal_cols = ['wday_num', 'month_num', 'year_norm', 'is_weekend',\n",
    "            'has_event_1', 'has_event_2', 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "\n",
    "cal_lookup = calendar.set_index('d')[cal_cols].values\n",
    "cal_d_to_idx = {d: i for i, d in enumerate(calendar['d'].values)}\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare Prices\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Preparing prices...\")\n",
    "prices['id'] = prices['item_id'] + '_' + prices['store_id'] + '_evaluation'\n",
    "prices_wide = prices.pivot_table(index='id', columns='wm_yr_wk', values='sell_price')\n",
    "prices_wide = prices_wide.ffill(axis=1).bfill(axis=1)\n",
    "cal_week_map = calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "\n",
    "# ============================================================================\n",
    "# Convert to Long Format - VECTORIZED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nConverting to long format (vectorized)...\")\n",
    "\n",
    "def prepare_m5_vectorized(sales_df, cal_lookup, cal_d_to_idx, prices_wide, cal_week_map, n_series=None):\n",
    "    if n_series:\n",
    "        sales_df = sales_df.head(n_series).copy()\n",
    "\n",
    "    date_cols = [c for c in sales_df.columns if c.startswith('d_')]\n",
    "    n_days = len(date_cols)\n",
    "    n_series_actual = len(sales_df)\n",
    "\n",
    "    day_indices = [cal_d_to_idx[d] for d in date_cols]\n",
    "    cal_matrix = cal_lookup[day_indices]\n",
    "    sales_matrix = sales_df[date_cols].values.astype(float)\n",
    "\n",
    "    price_matrix = np.ones((n_series_actual, n_days), dtype=float)\n",
    "    for i, series_id in enumerate(sales_df['id'].values):\n",
    "        if series_id in prices_wide.index:\n",
    "            series_prices = prices_wide.loc[series_id]\n",
    "            for j, d in enumerate(date_cols):\n",
    "                wk = cal_week_map.get(d)\n",
    "                if wk and wk in series_prices.index and pd.notna(series_prices[wk]):\n",
    "                    price_matrix[i, j] = series_prices[wk]\n",
    "\n",
    "    timestamps = pd.date_range(start='2011-01-29', periods=n_days, freq='D')\n",
    "    series_ids = np.repeat(sales_df['id'].values, n_days)\n",
    "    timestamp_arr = np.tile(timestamps, n_series_actual)\n",
    "    values = sales_matrix.flatten()\n",
    "    price_arr = price_matrix.flatten()\n",
    "    cal_tiled = np.tile(cal_matrix, (n_series_actual, 1))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': timestamp_arr,\n",
    "        'series_id': series_ids,\n",
    "        'value': values,\n",
    "        'price': price_arr,\n",
    "    })\n",
    "\n",
    "    for i, col in enumerate(cal_cols):\n",
    "        df[col] = cal_tiled[:, i]\n",
    "\n",
    "    valid_mask = df.groupby('series_id')['value'].transform('count') >= (CONTEXT_LENGTH + FORECAST_LENGTH)\n",
    "    df = df[valid_mask].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "data = prepare_m5_vectorized(sales, cal_lookup, cal_d_to_idx, prices_wide, cal_week_map, n_series=N_SERIES_SAMPLE)\n",
    "print(f\"  Data: {data.shape} | Series: {data['series_id'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Preprocessor\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nTraining preprocessor...\")\n",
    "\n",
    "covariate_cols = ['price', 'wday_num', 'month_num', 'year_norm', 'is_weekend',\n",
    "                  'has_event_1', 'has_event_2', 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": \"timestamp\",\n",
    "    \"id_columns\": [\"series_id\"],\n",
    "    \"target_columns\": [\"value\"],\n",
    "    \"control_columns\": covariate_cols,\n",
    "}\n",
    "\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    **column_specifiers,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    prediction_length=FORECAST_LENGTH,\n",
    "    scaling=True,\n",
    "    encode_categorical=False,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "train_end_date = data['timestamp'].quantile(0.8)\n",
    "df_train = data[data['timestamp'] <= train_end_date]\n",
    "trained_tsp = tsp.train(df_train)\n",
    "\n",
    "print(f\"  Channels: input={tsp.num_input_channels}, target={len(tsp.prediction_channel_indices)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Datasets\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nPreparing datasets...\")\n",
    "\n",
    "split_params = {\"train\": 0.6, \"test\": 0.2}\n",
    "train_data, valid_data, test_data = prepare_data_splits(\n",
    "    data,\n",
    "    id_columns=column_specifiers[\"id_columns\"],\n",
    "    split_config=split_params,\n",
    "    context_length=CONTEXT_LENGTH\n",
    ")\n",
    "\n",
    "min_length = CONTEXT_LENGTH + FORECAST_LENGTH + 2\n",
    "\n",
    "def filter_short(df, id_cols, min_len):\n",
    "    counts = df.groupby(id_cols).size()\n",
    "    valid = counts[counts >= min_len].index\n",
    "    return df[df[id_cols[0]].isin(valid)]\n",
    "\n",
    "train_data = filter_short(train_data, column_specifiers[\"id_columns\"], min_length)\n",
    "valid_data = filter_short(valid_data, column_specifiers[\"id_columns\"], min_length)\n",
    "\n",
    "frequency_token = tsp.get_frequency_token(tsp.freq)\n",
    "dataset_params = {\n",
    "    \"timestamp_column\": column_specifiers[\"timestamp_column\"],\n",
    "    \"id_columns\": column_specifiers[\"id_columns\"],\n",
    "    \"target_columns\": column_specifiers[\"target_columns\"],\n",
    "    \"control_columns\": column_specifiers[\"control_columns\"],\n",
    "    \"frequency_token\": frequency_token,\n",
    "    \"context_length\": CONTEXT_LENGTH,\n",
    "    \"prediction_length\": FORECAST_LENGTH,\n",
    "}\n",
    "\n",
    "train_dataset = ForecastDFDataset(tsp.preprocess(train_data), **dataset_params)\n",
    "valid_dataset = ForecastDFDataset(tsp.preprocess(valid_data), **dataset_params)\n",
    "\n",
    "n_train = len(train_dataset)\n",
    "train_dataset = Subset(train_dataset, np.random.permutation(n_train)[:int(FEWSHOT_FRACTION * n_train)])\n",
    "n_valid = len(valid_dataset)\n",
    "valid_dataset = Subset(valid_dataset, np.random.permutation(n_valid)[:int(FEWSHOT_FRACTION * n_valid)])\n",
    "\n",
    "print(f\"  Train: {len(train_dataset)} | Valid: {len(valid_dataset)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading TTM (multivariate)...\")\n",
    "\n",
    "finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    TTM_MODEL_PATH,\n",
    "    revision=REVISION,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    prediction_filter_length=FORECAST_LENGTH,\n",
    "    num_input_channels=tsp.num_input_channels,\n",
    "    prediction_channel_indices=tsp.prediction_channel_indices,\n",
    "    exogenous_channel_indices=list(range(len(tsp.prediction_channel_indices), tsp.num_input_channels)),\n",
    "    decoder_mode=\"mix_channel\",\n",
    "    enable_forecast_channel_mixing=True,\n",
    ")\n",
    "\n",
    "print(f\"  Parameters: {count_parameters(finetune_forecast_model):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "OUT_DIR = str(output_dir / \"training\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "finetune_forecast_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=2 * BATCH_SIZE,\n",
    "    dataloader_num_workers=1,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(finetune_forecast_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, LEARNING_RATE, epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / BATCH_SIZE),\n",
    ")\n",
    "\n",
    "finetune_forecast_trainer = Trainer(\n",
    "    model=finetune_forecast_model,\n",
    "    args=finetune_forecast_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        TrackingCallback(),\n",
    "    ],\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")\n",
    "\n",
    "finetune_forecast_trainer.train()\n",
    "valid_results = finetune_forecast_trainer.evaluate(valid_dataset)\n",
    "print(f\"\\nValidation Loss: {valid_results['eval_loss']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Forecasting - FIXED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nGenerating forecasts...\")\n",
    "\n",
    "finetune_forecast_model.eval()\n",
    "finetune_forecast_model.to(device)\n",
    "\n",
    "sales_full = pd.read_csv(data_dir / 'sales_train_evaluation.csv')\n",
    "series_order = sales_full['id'].tolist()\n",
    "\n",
    "print(\"  Preparing full data...\")\n",
    "data_full = prepare_m5_vectorized(sales_full, cal_lookup, cal_d_to_idx, prices_wide, cal_week_map, n_series=None)\n",
    "\n",
    "# Manual global scaler normalization\n",
    "print(\"  Normalizing...\")\n",
    "scaler_mean = float(tsp.scaler.mean_[0]) if hasattr(tsp, 'scaler') else 0.0\n",
    "scaler_scale = float(tsp.scaler.scale_[0]) if hasattr(tsp, 'scaler') else 1.0\n",
    "\n",
    "for col in ['value'] + covariate_cols:\n",
    "    if col in data_full.columns:\n",
    "        data_full[col] = (data_full[col] - scaler_mean) / scaler_scale\n",
    "\n",
    "print(f\"    Scaler: mean={scaler_mean:.2f}, scale={scaler_scale:.2f}\")\n",
    "\n",
    "# Batched inference\n",
    "freq_token = torch.tensor([frequency_token], dtype=torch.long, device=device)\n",
    "target_col = column_specifiers[\"target_columns\"][0]\n",
    "all_cols = [target_col] + covariate_cols\n",
    "INFERENCE_BATCH = 128\n",
    "\n",
    "all_forecasts_dict = {}\n",
    "series_groups = data_full.groupby('series_id')\n",
    "\n",
    "print(\"  Building batches...\")\n",
    "batch_tensors = []\n",
    "batch_series_ids = []\n",
    "\n",
    "for series_id in tqdm(series_order, desc=\"    Preparing\"):\n",
    "    if series_id in series_groups.groups:\n",
    "        group = series_groups.get_group(series_id).tail(CONTEXT_LENGTH)\n",
    "        if len(group) >= CONTEXT_LENGTH:\n",
    "            tensor = torch.from_numpy(group[all_cols].values[-CONTEXT_LENGTH:].astype(np.float32))\n",
    "            batch_tensors.append(tensor)\n",
    "            batch_series_ids.append(series_id)\n",
    "        else:\n",
    "            batch_tensors.append(None)\n",
    "            batch_series_ids.append(series_id)\n",
    "    else:\n",
    "        batch_tensors.append(None)\n",
    "        batch_series_ids.append(series_id)\n",
    "\n",
    "print(\"  Forecasting...\")\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(batch_tensors), INFERENCE_BATCH), desc=\"    Batches\"):\n",
    "        batch_slice = batch_tensors[i:i+INFERENCE_BATCH]\n",
    "        series_slice = batch_series_ids[i:i+INFERENCE_BATCH]\n",
    "        valid_idx = [j for j, t in enumerate(batch_slice) if t is not None]\n",
    "\n",
    "        if not valid_idx:\n",
    "            for sid in series_slice:\n",
    "                all_forecasts_dict[sid] = np.zeros(FORECAST_LENGTH)\n",
    "            continue\n",
    "\n",
    "        valid_tensors = torch.stack([batch_slice[j] for j in valid_idx]).to(device)\n",
    "        outputs = finetune_forecast_model(\n",
    "            past_values=valid_tensors,\n",
    "            freq_token=freq_token.expand(valid_tensors.shape[0])\n",
    "        )\n",
    "\n",
    "        if hasattr(outputs, 'prediction_outputs'):\n",
    "            forecasts = outputs.prediction_outputs.cpu().numpy()\n",
    "        elif hasattr(outputs, 'logits'):\n",
    "            forecasts = outputs.logits.cpu().numpy()\n",
    "        else:\n",
    "            forecasts = outputs.cpu().numpy()\n",
    "\n",
    "        if forecasts.ndim == 3:\n",
    "            forecasts = forecasts[:, :, 0]\n",
    "        \n",
    "        forecasts = forecasts[:, :FORECAST_LENGTH]\n",
    "        forecasts = forecasts * scaler_scale + scaler_mean\n",
    "        forecasts = np.maximum(forecasts, 0)\n",
    "\n",
    "        valid_iter = iter(range(len(valid_idx)))\n",
    "        for j, sid in enumerate(series_slice):\n",
    "            if j in valid_idx:\n",
    "                idx = next(valid_iter)\n",
    "                all_forecasts_dict[sid] = forecasts[idx]\n",
    "            else:\n",
    "                all_forecasts_dict[sid] = np.zeros(FORECAST_LENGTH)\n",
    "\n",
    "all_forecasts = [all_forecasts_dict[sid] for sid in series_order]\n",
    "forecast_array = np.array(all_forecasts)\n",
    "print(f\"  Forecast array: {forecast_array.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# WRMSSE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCalculating WRMSSE...\")\n",
    "\n",
    "sys.path.append('../src')\n",
    "try:\n",
    "    from m5_wrmsse import wrmsse\n",
    "    wrmsse_score = wrmsse(forecast_array)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  WRMSSE: {wrmsse_score:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    wrmsse_score = None\n",
    "\n",
    "# ============================================================================\n",
    "# Save\n",
    "# ============================================================================\n",
    "\n",
    "forecast_df = pd.DataFrame(forecast_array, index=series_order)\n",
    "forecast_df.to_pickle(output_dir / 'forecasts.pkl')\n",
    "\n",
    "summary = {\n",
    "    'wrmsse': wrmsse_score,\n",
    "    'valid_loss': valid_results['eval_loss'],\n",
    "    'n_series': len(series_order),\n",
    "    'context_length': CONTEXT_LENGTH,\n",
    "    'covariates': covariate_cols,\n",
    "}\n",
    "\n",
    "with open(output_dir / 'summary.pkl', 'wb') as f:\n",
    "    pickle.dump(summary, f)\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51103c00-55cc-4eea-be77-a3ca04eda027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTM new",
   "language": "python",
   "name": "ttm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
