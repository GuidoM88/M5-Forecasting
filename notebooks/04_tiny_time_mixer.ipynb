{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53bfc159-98fc-4dd6-849c-25fac303a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "\n",
      "Config:\n",
      "  Context: 512 giorni\n",
      "  Horizon: 28 giorni\n",
      "  Batch size: 128 (aumentato per velocità)\n",
      "  Max epochs: 10\n",
      "  Early stopping patience: 3\n",
      "  Device: mps\n",
      "\n",
      "[1/6] Caricamento dati...\n",
      "✓ Train: (58327370, 11)\n",
      "✓ Eval: (853720, 11)\n",
      "✓ Train pivot: (1913, 30490)\n",
      "\n",
      "[2/6] Preparazione dataset...\n",
      "  Creando windows da 30490 serie...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Processing: 100%|█████████████████████| 30490/30490 [00:25<00:00, 1194.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Created 1524500 training samples\n",
      "✓ Dataset ready: 1524500 samples\n",
      "  Batches per epoch: 11910\n",
      "\n",
      "[3/6] Definizione modello...\n",
      "✓ Modello creato: 241,372 params\n",
      "\n",
      "[4/6] Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████| 11911/11911 [01:08<00:00, 172.80it/s, loss=4.0293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Loss: 34458409384.0568 - LR: 0.001000\n",
      "  ✅ Best model saved (loss: 34458409384.0568)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████| 11911/11911 [01:07<00:00, 176.07it/s, loss=2.7698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Loss: 34458364809.2081 - LR: 0.001000\n",
      "  ✅ Best model saved (loss: 34458364809.2081)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████| 11911/11911 [01:08<00:00, 172.74it/s, loss=7.5165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Loss: 34458420115.6638 - LR: 0.001000\n",
      "  ⚠️  No improvement (1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█| 11911/11911 [01:10<00:00, 168.41it/s, loss=39285538816.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Loss: 34461181251.8401 - LR: 0.000500\n",
      "  ⚠️  No improvement (2/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████| 11911/11911 [01:09<00:00, 170.97it/s, loss=8.3934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Loss: 34458409815.1267 - LR: 0.000500\n",
      "  ⚠️  No improvement (3/3)\n",
      "\n",
      "🛑 Early stopping at epoch 5\n",
      "\n",
      "✓ Training completato! Best loss: 34458364809.2081, Best epoch: 2\n",
      "\n",
      "[5/6] Generazione forecasts...\n",
      "Serie totali: 30490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting: 100%|██████████████████████| 30490/30490 [00:26<00:00, 1165.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Forecast array: (30490, 28)\n",
      "\n",
      "[6/6] Calcolo WRMSSE...\n",
      "\n",
      "✅ WRMSSE: 0.9783\n",
      "\n",
      "✓ Summary salvato\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TTM \n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from m5_wrmsse import wrmsse\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "device = 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SETUP\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "OUTPUT_DIR = Path(\"../data/ttm_finetuned\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameters \n",
    "CONTEXT_LENGTH = 512\n",
    "FORECAST_HORIZON = 28\n",
    "BATCH_SIZE = 128          \n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 3\n",
    "MIN_DELTA = 0.001\n",
    "\n",
    "print(f\"\\nConfig:\")\n",
    "print(f\"  Context: {CONTEXT_LENGTH} giorni\")\n",
    "print(f\"  Horizon: {FORECAST_HORIZON} giorni\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (aumentato per velocità)\")\n",
    "print(f\"  Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Early stopping patience: {PATIENCE}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CARICA DATI\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1/6] Caricamento dati...\")\n",
    "\n",
    "with open(DATA_DIR / \"train_official.pkl\", 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(DATA_DIR / \"eval_official.pkl\", 'rb') as f:\n",
    "    eval_data = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Train: {train.shape}\")\n",
    "print(f\"✓ Eval: {eval_data.shape}\")\n",
    "\n",
    "# Pivot\n",
    "train_pivot = train.pivot(index='date', columns='id', values='sales')\n",
    "print(f\"✓ Train pivot: {train_pivot.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/6] Preparazione dataset...\")\n",
    "\n",
    "class M5TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, context_length, forecast_horizon):\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.data = data\n",
    "        self.samples = []\n",
    "        \n",
    "        print(f\"  Creando windows da {len(self.data.columns)} serie...\")\n",
    "        for col in tqdm(self.data.columns, desc=\"  Processing\"):\n",
    "            series = self.data[col].values\n",
    "            \n",
    "            if len(series) < context_length + forecast_horizon:\n",
    "                continue\n",
    "            \n",
    "            for i in range(0, len(series) - context_length - forecast_horizon + 1, 28):\n",
    "                context = series[i:i+context_length]\n",
    "                target = series[i+context_length:i+context_length+forecast_horizon]\n",
    "                \n",
    "                context_mean = context.mean()\n",
    "                context_std = context.std() + 1e-6\n",
    "                \n",
    "                context_norm = (context - context_mean) / context_std\n",
    "                target_norm = (target - context_mean) / context_std\n",
    "                \n",
    "                self.samples.append({\n",
    "                    'context': torch.FloatTensor(context_norm),\n",
    "                    'target': torch.FloatTensor(target_norm),\n",
    "                    'mean': context_mean,\n",
    "                    'std': context_std\n",
    "                })\n",
    "        \n",
    "        print(f\"  ✓ Created {len(self.samples)} training samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "train_dataset = M5TimeSeriesDataset(train_pivot, CONTEXT_LENGTH, FORECAST_HORIZON)\n",
    "print(f\"✓ Dataset ready: {len(train_dataset)} samples\")\n",
    "print(f\"  Batches per epoch: {len(train_dataset) // BATCH_SIZE}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MODELLO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/6] Definizione modello...\")\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, context_length, forecast_horizon, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(context_length, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 4, forecast_horizon)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        forecast = self.decoder(encoded)\n",
    "        return forecast\n",
    "\n",
    "model = TimeSeriesTransformer(CONTEXT_LENGTH, FORECAST_HORIZON).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"✓ Modello creato: {total_params:,} params\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRAINING CON EARLY STOPPING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/6] Training...\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses_history = []\n",
    "best_loss = float('inf')\n",
    "best_epoch = -1\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        context = batch['context'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses_history.append(avg_loss)\n",
    "    scheduler.step(avg_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Avg Loss: {avg_loss:.4f} - LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss - MIN_DELTA:\n",
    "        best_loss = avg_loss\n",
    "        best_epoch = epoch + 1  # Aggiorna best_epoch\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / 'best_model.pt')\n",
    "        print(f\"  ✅ Best model saved (loss: {best_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  ⚠️  No improvement ({patience_counter}/{PATIENCE})\")\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n🛑 Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n✓ Training completato! Best loss: {best_loss:.4f}, Best epoch: {best_epoch}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. FORECASTING E CALCOLO WRMSSE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/6] Generazione forecasts...\")\n",
    "\n",
    "model.load_state_dict(torch.load(OUTPUT_DIR / 'best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Carica ordine originale serie\n",
    "sales_orig = pd.read_csv(RAW_DIR / \"sales_train_evaluation.csv\")\n",
    "series_order = sales_orig['id'].tolist()\n",
    "\n",
    "print(f\"Serie totali: {len(series_order)}\")\n",
    "\n",
    "all_forecasts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for series_id in tqdm(series_order, desc=\"Forecasting\"):\n",
    "        if series_id in train_pivot.columns:\n",
    "            context = train_pivot[series_id].tail(CONTEXT_LENGTH).values\n",
    "            \n",
    "            context_mean = context.mean()\n",
    "            context_std = context.std() + 1e-6\n",
    "            context_norm = (context - context_mean) / context_std\n",
    "            \n",
    "            context_tensor = torch.FloatTensor(context_norm).unsqueeze(0).to(device)\n",
    "            output = model(context_tensor)\n",
    "            \n",
    "            forecast_norm = output.cpu().numpy().flatten()\n",
    "            forecast = forecast_norm * context_std + context_mean\n",
    "            forecast = np.maximum(forecast, 0)\n",
    "        else:\n",
    "            if series_id in train['id'].values:\n",
    "                last_val = train[train['id'] == series_id]['sales'].iloc[-1]\n",
    "                forecast = np.full(FORECAST_HORIZON, last_val)\n",
    "            else:\n",
    "                forecast = np.zeros(FORECAST_HORIZON)\n",
    "        \n",
    "        all_forecasts.append(forecast)\n",
    "\n",
    "forecast_array = np.array(all_forecasts)\n",
    "print(f\"✓ Forecast array: {forecast_array.shape}\")\n",
    "\n",
    "print(\"\\n[6/6] Calcolo WRMSSE...\")\n",
    "\n",
    "wrmsse_score = wrmsse(forecast_array)\n",
    "\n",
    "print(f\"\\n✅ WRMSSE: {wrmsse_score:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. SALVATAGGIO\n",
    "# ============================================================================\n",
    "\n",
    "forecast_df = pd.DataFrame(forecast_array, index=series_order)\n",
    "forecast_df.to_pickle(OUTPUT_DIR / 'ttm_forecasts.pkl')\n",
    "\n",
    "summary = {\n",
    "    'wrmsse': wrmsse_score,\n",
    "    'epochs_trained': len(losses_history),\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_loss': best_loss,\n",
    "    'n_series': len(series_order),\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'ttm_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(summary, f)\n",
    "\n",
    "print(\"\\n✓ Summary salvato\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b3649-a7d3-45f2-ad57-c44ded52530c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_env",
   "language": "python",
   "name": "mlops_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
