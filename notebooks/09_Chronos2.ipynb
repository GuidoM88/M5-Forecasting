{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4628a-d9a5-49c2-921a-a05517650340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronos-2 on M5 Forecasting\n",
    "# Zero-shot with covariates support\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "FORECAST_LENGTH = 28\n",
    "N_SERIES_TEST = None  \n",
    "data_dir = Path('../data/raw')\n",
    "output_dir = Path('../data/chronos_m5_output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Chronos-2 on M5\")\n",
    "print(f\"Forecast horizon: {FORECAST_LENGTH} days\")\n",
    "print(f\"Using all 30,490 series\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Chronos-2\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading Chronos-2 model...\")\n",
    "\n",
    "from chronos import Chronos2Pipeline\n",
    "\n",
    "# Use CPU or GPU\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load model\n",
    "pipeline = Chronos2Pipeline.from_pretrained(\n",
    "    \"amazon/chronos-2\",\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load M5 Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading M5 data...\")\n",
    "\n",
    "sales = pd.read_csv(data_dir / 'sales_train_evaluation.csv')\n",
    "calendar = pd.read_csv(data_dir / 'calendar.csv')\n",
    "prices = pd.read_csv(data_dir / 'sell_prices.csv')\n",
    "\n",
    "print(f\"  Sales: {sales.shape}\")\n",
    "print(f\"  Calendar: {calendar.shape}\")\n",
    "print(f\"  Prices: {prices.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare Data for Chronos-2\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nPreparing data...\")\n",
    "\n",
    "# Get date columns\n",
    "date_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "n_days = len(date_cols)\n",
    "timestamps = pd.date_range(start='2011-01-29', periods=n_days, freq='D')\n",
    "\n",
    "# Calendar features\n",
    "calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "calendar_dict = calendar.set_index('date').to_dict('index')\n",
    "\n",
    "def prepare_chronos_data(sales_df, calendar_df, n_series=None):\n",
    "    \"\"\"\n",
    "    Prepare M5 data in Chronos-2 format with covariates\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    - item_id: series identifier\n",
    "    - timestamp: date\n",
    "    - target: sales values\n",
    "    - covariate columns (calendar features)\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_series:\n",
    "        sales_df = sales_df.head(n_series)\n",
    "    \n",
    "    # Align calendar to sales days\n",
    "    date_cols = [c for c in sales_df.columns if c.startswith('d_')]\n",
    "    calendar_aligned = calendar_df[calendar_df['d'].isin(date_cols)].reset_index(drop=True)\n",
    "    \n",
    "    # Pre-compute calendar features\n",
    "    cal_wday = calendar_aligned['wday'].astype('category').cat.codes.values\n",
    "    cal_month = calendar_aligned['month'].astype('category').cat.codes.values\n",
    "    cal_weekend = calendar_aligned['weekday'].isin(['Saturday', 'Sunday']).astype(int).values\n",
    "    cal_snap_ca = calendar_aligned['snap_CA'].values\n",
    "    cal_snap_tx = calendar_aligned['snap_TX'].values\n",
    "    cal_snap_wi = calendar_aligned['snap_WI'].values\n",
    "    cal_event = calendar_aligned['event_name_1'].notna().astype(int).values\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for idx in tqdm(range(len(sales_df)), desc=\"  Converting\"):\n",
    "        series_id = sales_df.iloc[idx]['id']\n",
    "        sales_values = sales_df.iloc[idx][date_cols].values.astype(float)\n",
    "        \n",
    "        # Create DataFrame for this series\n",
    "        df_series = pd.DataFrame({\n",
    "            'item_id': series_id,\n",
    "            'timestamp': timestamps,\n",
    "            'target': sales_values,\n",
    "            'wday': cal_wday,\n",
    "            'month': cal_month,\n",
    "            'is_weekend': cal_weekend,\n",
    "            'snap_CA': cal_snap_ca,\n",
    "            'snap_TX': cal_snap_tx,\n",
    "            'snap_WI': cal_snap_wi,\n",
    "            'event': cal_event,\n",
    "        })\n",
    "        \n",
    "        all_data.append(df_series)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Convert all series\n",
    "if N_SERIES_TEST:\n",
    "    data_all = prepare_chronos_data(sales.head(N_SERIES_TEST), calendar)\n",
    "else:\n",
    "    data_all = prepare_chronos_data(sales, calendar)\n",
    "print(f\"  Data shape: {data_all.shape}\")\n",
    "print(f\"  Series: {data_all['item_id'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Zero-Shot Forecasting\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nGenerating forecasts (zero-shot)...\")\n",
    "\n",
    "# Split into context (history) and future\n",
    "context_end = timestamps[-FORECAST_LENGTH]\n",
    "context_df = data_all[data_all['timestamp'] <= context_end]\n",
    "\n",
    "print(f\"  Context data: {context_df.shape}\")\n",
    "print(f\"  Forecasting {FORECAST_LENGTH} steps ahead...\")\n",
    "print(f\"  Estimated time: ~4-6 hours for 30k series\")\n",
    "\n",
    "# Chronos-2 predict_df API\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "forecasts = pipeline.predict_df(\n",
    "    df=context_df,\n",
    "    prediction_length=FORECAST_LENGTH,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    target=\"target\",\n",
    "    quantile_levels=[0.1, 0.5, 0.9],  # Probabilistic forecasts\n",
    "    batch_size=128,  # Increased from 32 for speed\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"  Forecasting completed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "print(f\"  Forecast shape: {forecasts.shape}\")\n",
    "print(f\"  Forecast columns: {list(forecasts.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Extract Point Forecasts\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nExtracting point forecasts...\")\n",
    "\n",
    "# Get median (0.5 quantile) as point forecast\n",
    "if N_SERIES_TEST:\n",
    "    series_order = sales.head(N_SERIES_TEST)['id'].tolist()\n",
    "    print(f\"  NOTE: Using {N_SERIES_TEST} series (testing)\")\n",
    "else:\n",
    "    series_order = sales['id'].tolist()\n",
    "    print(f\"  Using all {len(series_order)} series\")\n",
    "\n",
    "all_forecasts = []\n",
    "\n",
    "# Determine the median column name\n",
    "if 'target_0.5' in forecasts.columns:\n",
    "    median_col = 'target_0.5'\n",
    "elif '0.5' in forecasts.columns:\n",
    "    median_col = '0.5'\n",
    "elif 'mean' in forecasts.columns:\n",
    "    median_col = 'mean'\n",
    "else:\n",
    "    # Use first numeric column after item_id and timestamp\n",
    "    numeric_cols = forecasts.select_dtypes(include=[np.number]).columns\n",
    "    median_col = [c for c in numeric_cols if c not in ['item_id']][0]\n",
    "\n",
    "print(f\"  Using column: {median_col}\")\n",
    "\n",
    "for series_id in tqdm(series_order, desc=\"  Processing\"):\n",
    "    series_forecast = forecasts[\n",
    "        (forecasts['item_id'] == series_id)\n",
    "    ].sort_values('timestamp')\n",
    "    \n",
    "    if len(series_forecast) > 0:\n",
    "        # Use median column\n",
    "        forecast_values = series_forecast[median_col].values[:FORECAST_LENGTH]\n",
    "        \n",
    "        # Pad if needed\n",
    "        if len(forecast_values) < FORECAST_LENGTH:\n",
    "            forecast_values = np.pad(\n",
    "                forecast_values,\n",
    "                (0, FORECAST_LENGTH - len(forecast_values)),\n",
    "                mode='edge'\n",
    "            )\n",
    "    else:\n",
    "        # Fallback: zeros\n",
    "        forecast_values = np.zeros(FORECAST_LENGTH)\n",
    "    \n",
    "    all_forecasts.append(forecast_values)\n",
    "\n",
    "forecast_array = np.array(all_forecasts)\n",
    "forecast_array = np.maximum(forecast_array, 0)  # No negative sales\n",
    "\n",
    "print(f\"  Final array: {forecast_array.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Calculate WRMSSE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCalculating WRMSSE...\")\n",
    "\n",
    "sys.path.append('../src')\n",
    "try:\n",
    "    from m5_wrmsse import wrmsse\n",
    "    wrmsse_score = wrmsse(forecast_array)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHRONOS-2 RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  WRMSSE: {wrmsse_score:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    wrmsse_score = None\n",
    "\n",
    "# ============================================================================\n",
    "# Save Results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nSaving results...\")\n",
    "\n",
    "# Save forecasts\n",
    "forecast_df = pd.DataFrame(forecast_array, index=series_order)\n",
    "forecast_df.to_pickle(output_dir / 'chronos_forecasts.pkl')\n",
    "\n",
    "# Save full probabilistic forecasts\n",
    "forecasts.to_pickle(output_dir / 'chronos_probabilistic.pkl')\n",
    "\n",
    "# Save summary\n",
    "import pickle\n",
    "summary = {\n",
    "    'wrmsse': wrmsse_score,\n",
    "    'model': 'chronos-2',\n",
    "    'n_series': len(series_order),\n",
    "    'forecast_length': FORECAST_LENGTH,\n",
    "    'zero_shot': True,\n",
    "}\n",
    "\n",
    "with open(output_dir / 'summary.pkl', 'wb') as f:\n",
    "    pickle.dump(summary, f)\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "print(f\"  Forecasts saved to: {output_dir}\")\n",
    "if wrmsse_score:\n",
    "    print(f\"  WRMSSE: {wrmsse_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd0e0c-a6c4-446a-bbe7-b2737143c362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c9d26-7354-4de3-bd00-1a88df8a13b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos_env",
   "language": "python",
   "name": "chronos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
